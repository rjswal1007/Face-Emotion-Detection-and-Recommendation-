{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38fcfe95-00f4-473a-a448-c43e50aa1d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "557eedaa-f04e-4c0b-bb7c-7a1265504951",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = 'images/train'\n",
    "TEST_DIR = 'images/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "455892f3-a03a-4753-bc51-fbadeb139894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createdataframe(dir):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for label in os.listdir(dir):\n",
    "        for imagename in os.listdir(os.path.join(dir,label)):\n",
    "            image_paths.append(os.path.join(dir,label,imagename))\n",
    "            labels.append(label)\n",
    "        print(label, \"completed\")\n",
    "    return image_paths,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7036a7bf-cb8c-477c-b05b-b2d08e8d9ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry completed\n",
      "disgust completed\n",
      "fear completed\n",
      "happy completed\n",
      "neutral completed\n",
      "sad completed\n",
      "surprise completed\n"
     ]
    }
   ],
   "source": [
    "train = pd.DataFrame()\n",
    "train['image'], train['label'] = createdataframe(TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a3997b9-b2a3-41b4-965d-fd1de80a0e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                image     label\n",
      "0            images/train\\angry\\0.jpg     angry\n",
      "1            images/train\\angry\\1.jpg     angry\n",
      "2           images/train\\angry\\10.jpg     angry\n",
      "3        images/train\\angry\\10002.jpg     angry\n",
      "4        images/train\\angry\\10016.jpg     angry\n",
      "...                               ...       ...\n",
      "28816  images/train\\surprise\\9969.jpg  surprise\n",
      "28817  images/train\\surprise\\9985.jpg  surprise\n",
      "28818  images/train\\surprise\\9990.jpg  surprise\n",
      "28819  images/train\\surprise\\9992.jpg  surprise\n",
      "28820  images/train\\surprise\\9996.jpg  surprise\n",
      "\n",
      "[28821 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1751649-48e5-49d0-a7af-a84131d2fd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry completed\n",
      "disgust completed\n",
      "fear completed\n",
      "happy completed\n",
      "neutral completed\n",
      "sad completed\n",
      "surprise completed\n"
     ]
    }
   ],
   "source": [
    "test = pd.DataFrame()\n",
    "test['image'], test['label'] = createdataframe(TEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6f2312a-2a80-479b-88e0-57ad9d6c321e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              image     label\n",
      "0       images/test\\angry\\10052.jpg     angry\n",
      "1       images/test\\angry\\10065.jpg     angry\n",
      "2       images/test\\angry\\10079.jpg     angry\n",
      "3       images/test\\angry\\10095.jpg     angry\n",
      "4       images/test\\angry\\10121.jpg     angry\n",
      "...                             ...       ...\n",
      "7061  images/test\\surprise\\9806.jpg  surprise\n",
      "7062  images/test\\surprise\\9830.jpg  surprise\n",
      "7063  images/test\\surprise\\9853.jpg  surprise\n",
      "7064  images/test\\surprise\\9878.jpg  surprise\n",
      "7065   images/test\\surprise\\993.jpg  surprise\n",
      "\n",
      "[7066 rows x 2 columns]\n",
      "0         images/test\\angry\\10052.jpg\n",
      "1         images/test\\angry\\10065.jpg\n",
      "2         images/test\\angry\\10079.jpg\n",
      "3         images/test\\angry\\10095.jpg\n",
      "4         images/test\\angry\\10121.jpg\n",
      "                    ...              \n",
      "7061    images/test\\surprise\\9806.jpg\n",
      "7062    images/test\\surprise\\9830.jpg\n",
      "7063    images/test\\surprise\\9853.jpg\n",
      "7064    images/test\\surprise\\9878.jpg\n",
      "7065     images/test\\surprise\\993.jpg\n",
      "Name: image, Length: 7066, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(test)\n",
    "print(test['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0f99beb-3572-4ca7-83be-8d467ff4cf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1895973-4a86-4036-afb8-db36c605be69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7902b347c3f64d509c0b14f0c189969a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28821 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2 as cv2\n",
    "def extract_features(images):\n",
    "    features = []\n",
    "    for image in tqdm(images):\n",
    "        # Load image (assuming path-like string)\n",
    "        img = cv2.imread(image)\n",
    "        # Check if image is loaded successfully\n",
    "        if img is None:\n",
    "            print(f\"Error loading image: {image}\")\n",
    "            continue\n",
    "        # Convert to grayscale (assuming BGR format)\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        # Convert to NumPy array\n",
    "        img_array = np.array(gray_img)\n",
    "        # Reshape to desired format (48x48x1)\n",
    "        img_array = img_array.reshape(48, 48, 1)  # Assuming target size is 48x48\n",
    "        features.append(img_array)\n",
    "    # Convert features list to NumPy array\n",
    "    features = np.array(features)\n",
    "    return features\n",
    "# Assuming 'train' is a Pandas DataFrame with an 'image' column containing paths\n",
    "train_features = extract_features(train['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28a0ddd9-4c6a-4a7e-9747-bb8ca5347441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a83b388ad2d4b12b15821b28e0eb0ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7066 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_features = extract_features(test['image'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18b7721e-ebb1-450e-9585-6b7a9e821079",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_features/255.0\n",
    "x_test = test_features/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "faabc68d-8cd3-47d4-b095-ca454dd53b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "497ccd84-c3f4-41c9-8c98-2fddf4374120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LabelEncoder<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.LabelEncoder.html\">?<span>Documentation for LabelEncoder</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LabelEncoder()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "406bbbc2-1071-4056-97b5-0fbb6cc85879",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = le.transform(train['label'])\n",
    "y_test = le.transform(test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb35251e-0342-43f4-8ff3-bbbc9aa5e786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train,num_classes = 7)\n",
    "y_test = to_categorical(y_test,num_classes = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a88651df-4e08-4741-af18-44c561d52d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Visual studio\\Python\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# convolutional layers\n",
    "model.add(Conv2D(128, kernel_size=(3,3), activation='relu', input_shape=(48,48,1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "# fully connected layers\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "# output layer\n",
    "model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04734ccb-e953-4609-91dc-aa985c78cffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd42f4a5-e2d3-4961-8446-94222bbadc7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m370s\u001b[0m 2s/step - accuracy: 0.2375 - loss: 1.8338 - val_accuracy: 0.2583 - val_loss: 1.8105\n",
      "Epoch 2/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 1s/step - accuracy: 0.2524 - loss: 1.8079 - val_accuracy: 0.2611 - val_loss: 1.7781\n",
      "Epoch 3/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 1s/step - accuracy: 0.2602 - loss: 1.7761 - val_accuracy: 0.2828 - val_loss: 1.7481\n",
      "Epoch 4/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 1s/step - accuracy: 0.3132 - loss: 1.6994 - val_accuracy: 0.3990 - val_loss: 1.5347\n",
      "Epoch 5/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 1s/step - accuracy: 0.3788 - loss: 1.5858 - val_accuracy: 0.4386 - val_loss: 1.4741\n",
      "Epoch 6/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 1s/step - accuracy: 0.4125 - loss: 1.5047 - val_accuracy: 0.4800 - val_loss: 1.3649\n",
      "Epoch 7/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 1s/step - accuracy: 0.4381 - loss: 1.4416 - val_accuracy: 0.5078 - val_loss: 1.3086\n",
      "Epoch 8/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 1s/step - accuracy: 0.4616 - loss: 1.3925 - val_accuracy: 0.5201 - val_loss: 1.2744\n",
      "Epoch 9/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m335s\u001b[0m 1s/step - accuracy: 0.4803 - loss: 1.3570 - val_accuracy: 0.5279 - val_loss: 1.2494\n",
      "Epoch 10/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 1s/step - accuracy: 0.4893 - loss: 1.3276 - val_accuracy: 0.5415 - val_loss: 1.2172\n",
      "Epoch 11/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 1s/step - accuracy: 0.5022 - loss: 1.2997 - val_accuracy: 0.5470 - val_loss: 1.1970\n",
      "Epoch 12/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 1s/step - accuracy: 0.5085 - loss: 1.2888 - val_accuracy: 0.5558 - val_loss: 1.1750\n",
      "Epoch 13/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 1s/step - accuracy: 0.5162 - loss: 1.2687 - val_accuracy: 0.5589 - val_loss: 1.1573\n",
      "Epoch 14/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 1s/step - accuracy: 0.5247 - loss: 1.2472 - val_accuracy: 0.5676 - val_loss: 1.1485\n",
      "Epoch 15/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 1s/step - accuracy: 0.5308 - loss: 1.2372 - val_accuracy: 0.5651 - val_loss: 1.1412\n",
      "Epoch 16/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 1s/step - accuracy: 0.5366 - loss: 1.2240 - val_accuracy: 0.5686 - val_loss: 1.1434\n",
      "Epoch 17/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 1s/step - accuracy: 0.5329 - loss: 1.2172 - val_accuracy: 0.5689 - val_loss: 1.1424\n",
      "Epoch 18/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 1s/step - accuracy: 0.5447 - loss: 1.1854 - val_accuracy: 0.5863 - val_loss: 1.1092\n",
      "Epoch 19/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 1s/step - accuracy: 0.5522 - loss: 1.1804 - val_accuracy: 0.5843 - val_loss: 1.1159\n",
      "Epoch 20/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m335s\u001b[0m 1s/step - accuracy: 0.5577 - loss: 1.1738 - val_accuracy: 0.5793 - val_loss: 1.1106\n",
      "Epoch 21/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 1s/step - accuracy: 0.5485 - loss: 1.1729 - val_accuracy: 0.5788 - val_loss: 1.1268\n",
      "Epoch 22/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 1s/step - accuracy: 0.5595 - loss: 1.1734 - val_accuracy: 0.5851 - val_loss: 1.1023\n",
      "Epoch 23/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 1s/step - accuracy: 0.5697 - loss: 1.1361 - val_accuracy: 0.5903 - val_loss: 1.0884\n",
      "Epoch 24/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 1s/step - accuracy: 0.5658 - loss: 1.1373 - val_accuracy: 0.5851 - val_loss: 1.1022\n",
      "Epoch 25/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 1s/step - accuracy: 0.5718 - loss: 1.1282 - val_accuracy: 0.5858 - val_loss: 1.1004\n",
      "Epoch 26/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 1s/step - accuracy: 0.5812 - loss: 1.1061 - val_accuracy: 0.5941 - val_loss: 1.0884\n",
      "Epoch 27/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 1s/step - accuracy: 0.5811 - loss: 1.1048 - val_accuracy: 0.5998 - val_loss: 1.0663\n",
      "Epoch 28/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m335s\u001b[0m 1s/step - accuracy: 0.5864 - loss: 1.0988 - val_accuracy: 0.5992 - val_loss: 1.0833\n",
      "Epoch 29/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 1s/step - accuracy: 0.5895 - loss: 1.0873 - val_accuracy: 0.5985 - val_loss: 1.0753\n",
      "Epoch 30/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 1s/step - accuracy: 0.5839 - loss: 1.0894 - val_accuracy: 0.6005 - val_loss: 1.0705\n",
      "Epoch 31/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 1s/step - accuracy: 0.5864 - loss: 1.0835 - val_accuracy: 0.6033 - val_loss: 1.0586\n",
      "Epoch 32/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 1s/step - accuracy: 0.5917 - loss: 1.0710 - val_accuracy: 0.6056 - val_loss: 1.0600\n",
      "Epoch 33/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 1s/step - accuracy: 0.6017 - loss: 1.0658 - val_accuracy: 0.6053 - val_loss: 1.0614\n",
      "Epoch 34/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 1s/step - accuracy: 0.5954 - loss: 1.0618 - val_accuracy: 0.6056 - val_loss: 1.0596\n",
      "Epoch 35/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 1s/step - accuracy: 0.6013 - loss: 1.0538 - val_accuracy: 0.6097 - val_loss: 1.0505\n",
      "Epoch 36/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m347s\u001b[0m 2s/step - accuracy: 0.6097 - loss: 1.0339 - val_accuracy: 0.6117 - val_loss: 1.0534\n",
      "Epoch 37/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m335s\u001b[0m 1s/step - accuracy: 0.6130 - loss: 1.0279 - val_accuracy: 0.6169 - val_loss: 1.0451\n",
      "Epoch 38/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 1s/step - accuracy: 0.6139 - loss: 1.0236 - val_accuracy: 0.6076 - val_loss: 1.0512\n",
      "Epoch 39/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 1s/step - accuracy: 0.6148 - loss: 1.0146 - val_accuracy: 0.6084 - val_loss: 1.0515\n",
      "Epoch 40/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 1s/step - accuracy: 0.6170 - loss: 1.0094 - val_accuracy: 0.6129 - val_loss: 1.0438\n",
      "Epoch 41/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 1s/step - accuracy: 0.6188 - loss: 1.0140 - val_accuracy: 0.6148 - val_loss: 1.0383\n",
      "Epoch 42/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m335s\u001b[0m 1s/step - accuracy: 0.6217 - loss: 0.9969 - val_accuracy: 0.6185 - val_loss: 1.0384\n",
      "Epoch 43/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 1s/step - accuracy: 0.6222 - loss: 0.9954 - val_accuracy: 0.6227 - val_loss: 1.0259\n",
      "Epoch 44/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 1s/step - accuracy: 0.6340 - loss: 0.9787 - val_accuracy: 0.6183 - val_loss: 1.0427\n",
      "Epoch 45/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 1s/step - accuracy: 0.6359 - loss: 0.9767 - val_accuracy: 0.6199 - val_loss: 1.0209\n",
      "Epoch 46/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m339s\u001b[0m 2s/step - accuracy: 0.6335 - loss: 0.9716 - val_accuracy: 0.6124 - val_loss: 1.0361\n",
      "Epoch 47/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 1s/step - accuracy: 0.6420 - loss: 0.9566 - val_accuracy: 0.6094 - val_loss: 1.0387\n",
      "Epoch 48/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 1s/step - accuracy: 0.6397 - loss: 0.9647 - val_accuracy: 0.6182 - val_loss: 1.0412\n",
      "Epoch 49/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 1s/step - accuracy: 0.6450 - loss: 0.9568 - val_accuracy: 0.6206 - val_loss: 1.0328\n",
      "Epoch 50/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 2s/step - accuracy: 0.6475 - loss: 0.9402 - val_accuracy: 0.6196 - val_loss: 1.0343\n",
      "Epoch 51/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 1s/step - accuracy: 0.6430 - loss: 0.9474 - val_accuracy: 0.6163 - val_loss: 1.0416\n",
      "Epoch 52/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m335s\u001b[0m 1s/step - accuracy: 0.6526 - loss: 0.9401 - val_accuracy: 0.6235 - val_loss: 1.0274\n",
      "Epoch 53/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 1s/step - accuracy: 0.6507 - loss: 0.9277 - val_accuracy: 0.6182 - val_loss: 1.0335\n",
      "Epoch 54/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 1s/step - accuracy: 0.6492 - loss: 0.9417 - val_accuracy: 0.6267 - val_loss: 1.0199\n",
      "Epoch 55/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 1s/step - accuracy: 0.6498 - loss: 0.9231 - val_accuracy: 0.6183 - val_loss: 1.0332\n",
      "Epoch 56/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 1s/step - accuracy: 0.6526 - loss: 0.9338 - val_accuracy: 0.6216 - val_loss: 1.0328\n",
      "Epoch 57/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 1s/step - accuracy: 0.6628 - loss: 0.8970 - val_accuracy: 0.6176 - val_loss: 1.0362\n",
      "Epoch 58/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 1s/step - accuracy: 0.6587 - loss: 0.9119 - val_accuracy: 0.6185 - val_loss: 1.0376\n",
      "Epoch 59/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 1s/step - accuracy: 0.6608 - loss: 0.9073 - val_accuracy: 0.6284 - val_loss: 1.0267\n",
      "Epoch 60/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 1s/step - accuracy: 0.6593 - loss: 0.9068 - val_accuracy: 0.6213 - val_loss: 1.0275\n",
      "Epoch 61/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 1s/step - accuracy: 0.6651 - loss: 0.8902 - val_accuracy: 0.6295 - val_loss: 1.0217\n",
      "Epoch 62/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 1s/step - accuracy: 0.6726 - loss: 0.8807 - val_accuracy: 0.6292 - val_loss: 1.0169\n",
      "Epoch 63/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 1s/step - accuracy: 0.6651 - loss: 0.8913 - val_accuracy: 0.6230 - val_loss: 1.0248\n",
      "Epoch 64/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 1s/step - accuracy: 0.6758 - loss: 0.8685 - val_accuracy: 0.6262 - val_loss: 1.0192\n",
      "Epoch 65/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 1s/step - accuracy: 0.6793 - loss: 0.8674 - val_accuracy: 0.6202 - val_loss: 1.0205\n",
      "Epoch 66/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 1s/step - accuracy: 0.6737 - loss: 0.8850 - val_accuracy: 0.6289 - val_loss: 1.0215\n",
      "Epoch 67/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 1s/step - accuracy: 0.6846 - loss: 0.8634 - val_accuracy: 0.6245 - val_loss: 1.0327\n",
      "Epoch 68/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 1s/step - accuracy: 0.6813 - loss: 0.8601 - val_accuracy: 0.6231 - val_loss: 1.0237\n",
      "Epoch 69/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 1s/step - accuracy: 0.6843 - loss: 0.8552 - val_accuracy: 0.6306 - val_loss: 1.0176\n",
      "Epoch 70/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 1s/step - accuracy: 0.6806 - loss: 0.8596 - val_accuracy: 0.6303 - val_loss: 1.0208\n",
      "Epoch 71/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 1s/step - accuracy: 0.6844 - loss: 0.8458 - val_accuracy: 0.6327 - val_loss: 1.0200\n",
      "Epoch 72/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 1s/step - accuracy: 0.6926 - loss: 0.8322 - val_accuracy: 0.6278 - val_loss: 1.0234\n",
      "Epoch 73/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 1s/step - accuracy: 0.6948 - loss: 0.8313 - val_accuracy: 0.6339 - val_loss: 1.0188\n",
      "Epoch 74/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m339s\u001b[0m 2s/step - accuracy: 0.6917 - loss: 0.8378 - val_accuracy: 0.6319 - val_loss: 1.0215\n",
      "Epoch 75/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 1s/step - accuracy: 0.6975 - loss: 0.8297 - val_accuracy: 0.6339 - val_loss: 1.0266\n",
      "Epoch 76/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 1s/step - accuracy: 0.6951 - loss: 0.8240 - val_accuracy: 0.6282 - val_loss: 1.0355\n",
      "Epoch 77/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 1s/step - accuracy: 0.6940 - loss: 0.8351 - val_accuracy: 0.6316 - val_loss: 1.0217\n",
      "Epoch 78/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 1s/step - accuracy: 0.6955 - loss: 0.8157 - val_accuracy: 0.6318 - val_loss: 1.0284\n",
      "Epoch 79/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 1s/step - accuracy: 0.7027 - loss: 0.8042 - val_accuracy: 0.6234 - val_loss: 1.0342\n",
      "Epoch 80/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m336s\u001b[0m 1s/step - accuracy: 0.7040 - loss: 0.7968 - val_accuracy: 0.6286 - val_loss: 1.0215\n",
      "Epoch 81/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 1s/step - accuracy: 0.7033 - loss: 0.8111 - val_accuracy: 0.6383 - val_loss: 1.0219\n",
      "Epoch 82/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m337s\u001b[0m 1s/step - accuracy: 0.7027 - loss: 0.8047 - val_accuracy: 0.6298 - val_loss: 1.0274\n",
      "Epoch 83/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 1s/step - accuracy: 0.7021 - loss: 0.8123 - val_accuracy: 0.6320 - val_loss: 1.0352\n",
      "Epoch 84/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 1s/step - accuracy: 0.7098 - loss: 0.7826 - val_accuracy: 0.6349 - val_loss: 1.0258\n",
      "Epoch 85/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m339s\u001b[0m 1s/step - accuracy: 0.7092 - loss: 0.7809 - val_accuracy: 0.6332 - val_loss: 1.0260\n",
      "Epoch 86/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 1s/step - accuracy: 0.7078 - loss: 0.8020 - val_accuracy: 0.6289 - val_loss: 1.0328\n",
      "Epoch 87/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 1s/step - accuracy: 0.7127 - loss: 0.7866 - val_accuracy: 0.6377 - val_loss: 1.0198\n",
      "Epoch 88/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m346s\u001b[0m 2s/step - accuracy: 0.7165 - loss: 0.7817 - val_accuracy: 0.6326 - val_loss: 1.0287\n",
      "Epoch 89/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m339s\u001b[0m 2s/step - accuracy: 0.7153 - loss: 0.7748 - val_accuracy: 0.6394 - val_loss: 1.0274\n",
      "Epoch 90/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 1s/step - accuracy: 0.7139 - loss: 0.7730 - val_accuracy: 0.6353 - val_loss: 1.0356\n",
      "Epoch 91/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 1s/step - accuracy: 0.7203 - loss: 0.7621 - val_accuracy: 0.6359 - val_loss: 1.0234\n",
      "Epoch 92/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 1s/step - accuracy: 0.7182 - loss: 0.7627 - val_accuracy: 0.6315 - val_loss: 1.0329\n",
      "Epoch 93/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 1s/step - accuracy: 0.7245 - loss: 0.7653 - val_accuracy: 0.6378 - val_loss: 1.0340\n",
      "Epoch 94/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m338s\u001b[0m 1s/step - accuracy: 0.7309 - loss: 0.7499 - val_accuracy: 0.6397 - val_loss: 1.0137\n",
      "Epoch 95/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 1s/step - accuracy: 0.7265 - loss: 0.7513 - val_accuracy: 0.6340 - val_loss: 1.0233\n",
      "Epoch 96/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m335s\u001b[0m 1s/step - accuracy: 0.7304 - loss: 0.7458 - val_accuracy: 0.6335 - val_loss: 1.0329\n",
      "Epoch 97/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 1s/step - accuracy: 0.7285 - loss: 0.7432 - val_accuracy: 0.6326 - val_loss: 1.0422\n",
      "Epoch 98/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 1s/step - accuracy: 0.7266 - loss: 0.7429 - val_accuracy: 0.6343 - val_loss: 1.0419\n",
      "Epoch 99/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 1s/step - accuracy: 0.7330 - loss: 0.7416 - val_accuracy: 0.6352 - val_loss: 1.0292\n",
      "Epoch 100/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 1s/step - accuracy: 0.7348 - loss: 0.7420 - val_accuracy: 0.6344 - val_loss: 1.0381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x195ff5b03d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x= x_train,y = y_train, batch_size = 128, epochs = 100, validation_data = (x_test,y_test)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58920911-f427-4d60-bb7e-a7bcebc632d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0e6282e-28ff-43c4-a737-27f4bc43add6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"emotiondetector.json\",'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save(\"emotiondetector.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6c6ffd5-1c98-4452-9e7c-9d653f8a2dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open(\"emotiondetector.json\", \"r\")\n",
    "model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(model_json)\n",
    "model.load_weights(\"emotiondetector.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04c4a13e-066a-46bb-9a2e-270169a31111",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['angry','disgust','fear','happy','neutral','sad','surprise']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96dfbbb0-4ed9-43a8-878b-73543da21027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ef(image):\n",
    "    img = load_img(image,grayscale =  True )\n",
    "    feature = np.array(img)\n",
    "    feature = feature.reshape(1,48,48,1)\n",
    "    return feature/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c4cb3fe-97c1-4453-a9ef-8961476f850e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image is labeled as sad\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step\n",
      "Model prediction: sad\n"
     ]
    }
   ],
   "source": [
    "def preprocess_image(image_path, target_size=(48, 48)):\n",
    "    try:\n",
    "        img = cv2.imread(image_path)  # Load image using OpenCV\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error loading image: {image_path} ({e})\") from e\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Image not found at: {image_path}\")\n",
    "   # Resize and convert to grayscale (combined)\n",
    "    img = cv2.cvtColor(cv2.resize(img, target_size), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Reshape to match the expected input shape of your model (batch size included)\n",
    "    img = img.reshape(1, *target_size, 1)  # Efficient reshaping with unpacking\n",
    "\n",
    "    # Normalize pixel values (optional, adjust based on your model's preprocessing)\n",
    "    # Example: Normalize to range [0, 1]\n",
    "    img = img.astype('float32') / 255.0\n",
    "\n",
    "    return img\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = 'images/train/sad/42.jpg'\n",
    "    print(\"Original image is labeled as sad\")  # Assuming labels are provided\n",
    "\n",
    "    try:\n",
    "        preprocessed_img = preprocess_image(image_path)\n",
    "        pred = model.predict(preprocessed_img)\n",
    "        pred_label = label[np.argmax(pred)]\n",
    "        print(\"Model prediction:\", pred_label)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14bee2af-8734-4ff8-901e-cf6e68a3deff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in d:\\visual studio\\python\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: numpy>=1.21 in d:\\visual studio\\python\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\visual studio\\python\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\visual studio\\python\\lib\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\visual studio\\python\\lib\\site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\visual studio\\python\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\visual studio\\python\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\visual studio\\python\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in d:\\visual studio\\python\\lib\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\visual studio\\python\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\visual studio\\python\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac3050f5-2ad9-49d3-9feb-7e2c436597f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Model prediction: sad\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x196032c1ae0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_image(image_path, target_size=(48, 48)):\n",
    "    try:\n",
    "        img = cv2.imread(image_path)  # Load image using OpenCV\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error loading image: {image_path} ({e})\") from e\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Image not found at: {image_path}\")\n",
    "   # Resize and convert to grayscale (combined)\n",
    "    img = cv2.cvtColor(cv2.resize(img, target_size), cv2.COLOR_BGR2GRAY)\n",
    "    img = img.reshape(1, *target_size, 1)  # Efficient reshaping with unpacking\n",
    "    img = img.astype('float32') / 255.0\n",
    "    return img\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = 'D:\\LPU Study material\\(4) Fourth Year\\Sem 8\\Capstone\\Face Emotion Detection\\RJ.jpg'\n",
    "    try:\n",
    "        preprocessed_img = preprocess_image(image_path)\n",
    "        pred = model.predict(preprocessed_img)\n",
    "        pred_label = label[np.argmax(pred)]\n",
    "        print(\"Model prediction:\", pred_label)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "plt.imshow(preprocessed_img.reshape(48,48),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83ada49-6bd3-4ada-9d95-d01894d82475",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
